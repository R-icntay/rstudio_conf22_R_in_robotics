[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A touch of R in robotics",
    "section": "",
    "text": "Hujambo and welcome to the accompanying guide for our rstudio::conf(2022) talk: A Touch of R in Robotics.\nIn these series of notebooks, we try as much as possible to provide detailed explanations of the methodologies used along with the corresponding code.\nWe hope you enjoy it as much as we did 🤖.\n\n\n\n\n\n\nNote\n\n\n\nHujambo is a Swahili word used in the context of Hello or How are you?. One typically replies with Sijambo meaning I am fine"
  },
  {
    "objectID": "01_speech_2_text_via_azure.html",
    "href": "01_speech_2_text_via_azure.html",
    "title": "1  Speech to text",
    "section": "",
    "text": "This notebook contains a step by step guide on how to record audio in R using the audio package and convert the recording to text using Microsoft Azure Cognitive Services, specifically the speech-to-text service."
  },
  {
    "objectID": "01_speech_2_text_via_azure.html#introduction",
    "href": "01_speech_2_text_via_azure.html#introduction",
    "title": "1  Speech to text",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nI recently found myself working on a chess play automation project. The user would issue a voice command describing the location of the piece they want to move and their desired destination location, the data would then processed and a chess move made. This was so cool and being the R-nista that I am (i don’t think this is a word, i have only come across Pythonistas ) ,I thought to myself, maybe this can also be done in R and voila, this article was born. For any feedback feel free to reach out at Ian"
  },
  {
    "objectID": "01_speech_2_text_via_azure.html#recording-audio-in-r-using-the-audio-package",
    "href": "01_speech_2_text_via_azure.html#recording-audio-in-r-using-the-audio-package",
    "title": "1  Speech to text",
    "section": "1.2 Recording Audio in R using the audio Package",
    "text": "1.2 Recording Audio in R using the audio Package\nWe begin with the input which we plan to transcribe. In order to record audio,we need to install and load the audio package\n\n#install the audio package\ninstall.packages(\"audio\")\n\nWe then proceed to setup the audio driver which we will use to record as shown below:\n\n#Load the library\nlibrary(audio)\n#Check which audio driver is set\ncurrent.audio.driver()\n\n\n#if there is no driver set, view what drivers are available\naudio.drivers()\n\n\n#from the list provided, set which driver to use (default driver is always best)\nset.audio.driver(NULL)# sets the default audio driver\n#option 2\nset.audio.driver (insert_name_here ) #sets the audio driver to a driver other than the default\n\n\n#Checks and verifies that indeed the audio driver is set as per the command above\ncurrent.audio.driver() \n\n\n1.2.1 Recording Audio\nSome of the key parameters in audio processing which we are going to encounter during this exercise include:\nSample rate: This is the number of times per second a sound is sampled and recorded.Therefore, if we use a sampling rate of say 8000 Hertz, a recording with a duration of 5 seconds will contain 40,000 samples (8000 * 5). The industry standard sampling rate commonly used is 44100 Hertz.\nMono vs Stereo: If you are a sound enthusiast, you’ve probably come across these terms. Simply put, Mono sound is recorded and played back using only one audio channel e.g. a guitarist recording using one mic to pick up sound of the guitar and Stereo sound is recorded and played through more than one channel.\nIn our case we will use one audio channel(mono) and a sampling rate of 44100Hz. That being said let’s start our recording\n\n#Set our recording time\nrec_time <- 5 \n\n#Recording\nSamples<-rep(NA_real_, 44100 * rec_time) #Defining number of samples recorded\nprint(\"Start speaking\")\naudio_obj <-record(Samples, 44100, 1) #Create an audio instance with sample rate 44100 and mono channel\nwait(6)\nrec <- audio_obj$data # get the recorded data from the audio object\n \n#Save the recorded audio\nfile.create(\"sample.wav\")#gets created in your current working directory \nsave.wave(rec,\"Insert_path_to_sample.wav_here\")\n\nOn recording and saving the audio file to sample.wav , we have to clear the audio instance object audio_obj before proceeding to make the next recording. From the audio package documentation, this is achieved by using the close(con,…) method, where con is the audioInstance object .However, using this method proved to be cumbersome as it causes the console to freeze anytime you want to record audio for more than one time. After doing some research, I discovered that restarting the console clears the audio instance object therefore allowing for one to record audio multiple times with no issue. I know this is not an elegant solution (more like tying duct tape around a leaking pipe )and i am actively looking for a better solution to fix this issue. For now, we go by the saying: if it works, don’t touch it and implement this step to clear the audio instance object.\n\n.rs.restartR() # clear the audio instance object(looking for a more elegant solution)\nwait(3)\n\nPlay the recording we just created just to confirm that indeed we recorded something.\n\nplay(rec)\n\nThat’s it for our input. We now proceed to process this and transcribe the audio we just recorded"
  },
  {
    "objectID": "01_speech_2_text_via_azure.html#house-keeping-on-azure",
    "href": "01_speech_2_text_via_azure.html#house-keeping-on-azure",
    "title": "1  Speech to text",
    "section": "1.3 House Keeping on Azure",
    "text": "1.3 House Keeping on Azure\nFirst we begin with some light housekeeping, i.e setting up a Cognitive Services resource in our Azure subscription. You can create a free Azure account here and if you have an account already, you can skip this step.\nWe then proceed to create a cognitive resource group by following these steps:\\\\ 1. Open the Azure portal https://portal.azure.com and sign in using your Microsoft account.\\\\ 2. Click + Create a resource and on the search bar, type Speech, click create and create a Cognitive service resource with the following settings:\n\nSubscription: Enter your Azure subscription\nResource group: Create one with a unique name or an existing one\nRegion: Choose any\nName: Enter a unique name\nPricing tier: Standard S0\nSelect I have read and understood the notices\nClick on Review + create and we are good to go.\n\nAs a guide, these are the settings I used however, you can go ahead and get creative with the names."
  },
  {
    "objectID": "01_speech_2_text_via_azure.html#speech-to-text-api-call",
    "href": "01_speech_2_text_via_azure.html#speech-to-text-api-call",
    "title": "1  Speech to text",
    "section": "1.4 Speech-to-text API call",
    "text": "1.4 Speech-to-text API call\nSince now we have our audio sample input sample.wav ,we can now go ahead to the transcribing bit. It is noteworthy that the speech to text rest API for short audio has several limitations which are:\n\nRequests cannot contain more than 60 seconds of audio. For batch transcriptions and custom speech use Speech to Text API v3.0\nThe API does not provide partial results.\nIt does not support Speech translation. For translation, you can checkout the Speech SDK\n\nNow that we are done with that, we can then go ahead and get coding.\nNote: To access the speech service we just created from R, we need the URL of the endpoint, location/region details and KEY 1 & 2 parameters. These can be found in the azure portal under Keys and Endpoint page of your cognitive service resource as illustrated below:\n\nCopy these details and save them since we are going to need them when make our API call. Note: you can use either KEY 1 or KEY 2 as your subscription key\nTo begin our transcription, install the packages we are going to need if you don’t have them already installed:\n\n#Enables us to work with HTTP in R\ninstall.packages(\"httr\") \ninstall.packages(\"jsonlite\")\n\nNote You will have to modify the subscription key, language and data path parameters to match the ones on the Cognitive Services resource you created earlier. Also modify the URL 'https://eastus.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1' by inserting the aforementioned location details as shown in the modified URL : 'https://_ENTER_LOCATION_HERE_.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1' . At the end you should have something like this:\n\nlibrary(httr)\nlibrary(jsonlite)  \n#Documentation on the two packages can be found by running ?httr and ?jsonlite commands respectively on the console.\n\n#Define headers containing subscription key and content type\nheaders = c(\n  `Ocp-Apim-Subscription-Key` = '_ENTER_YOUR_SUBSCRIPTION_KEY_HERE',  #Key 1 or 2\n  `Content-Type` = 'audio/wav' #Since we are transcribing a WAV file\n)\n\n#Create a parameters list, in this case we specify the languange parameter\nparams = list(\n  `language` = 'en-US'\n)\n\n#Enter path to the audio file we just recorded and saved\ndata = upload_file ('Insert_path_to_sample.wav_here') \n\n#Make the API call and save the response recived\nresponse <- httr::POST(url = 'https://eastus.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1', \nhttr::add_headers(.headers=headers), query = params, body = data)\n\n#Convert response received to a dataframe\nresult <- fromJSON(content(response, as  = 'text')) \ntxt_output <- as.data.frame(result)\n\n#Extract transcribed text\ntxt_input <- txt_output[1,2]\ntxt_input\n\nAnd with that, we have successfully recorded audio and transcribed it with the aid of Microsoft Azure Cognitive Services. The applications of the cognitive services are wide and this is just but a glimpse of what one can accomplish using the Speech to Text service. I’ll leave it at that. Thank you foR youR time. Cheers."
  },
  {
    "objectID": "02_kinematics.html",
    "href": "02_kinematics.html",
    "title": "2  Manipulator kinematics",
    "section": "",
    "text": "This Notebook illustrates how the manipulator will arrive at the desired location as instructed by the speech input."
  },
  {
    "objectID": "02_kinematics.html#forward-and-inverse-kinematics",
    "href": "02_kinematics.html#forward-and-inverse-kinematics",
    "title": "2  Manipulator kinematics",
    "section": "2.1 Forward and Inverse Kinematics",
    "text": "2.1 Forward and Inverse Kinematics\nKinematics is the science of motion that treats the subject without regard to the forces that cause it.\n\n2.1.1 Forward Kinematics\nForward kinematics addresses the problem of computing the position and orientation of the end effector relative to the user’s workstation given the joint angles of the manipulator.\nThe forward kinematics were performed in accordance with the Denavit—Hartenberg (D-H) convention. According to the D-H convention, any robot can be described kinematically by giving the values of four quantities, typically known as the D-H parameters, for each link. The link length a and the link twist \\alphaquantities, describe the link itself and the remaining two; link offset d and the joint angle \\theta describe the link’s connection to a neighboring link.\nTo perform the manipulator kinematics, link frames were attached to the manipulator as shown in Figure 2.1. In summary, link frames are laid out as follows:\n\nThe z-axis is in the direction of the joint axis.\nThe x-axis is parallel to the common normal.\nThe y-axis follows from the x-axis and z-axis by choosing it to be a right-handed coordinate system.\n\n\n\n\n\n\n\n\nmeArm parallel-link manipulator\n\n\n\n\nFigure 2.1: \n\n\nOnce the link frames have been laid, the D-H parameters can be easily defined as:\nd: offset along the previous z to the common normal.\n\\theta: angle about previous z, from old x to new x.\na: length of the common normal.\n\\alpha: angle about common normal from old z\\ axis to new z\\ axis.\nThe D-H parameters for the meArm were evaluated as below:\n\nD-H parameters for meArm\n\n\nLink\n\\theta\na(mm)\n\\alpha\nd(mm)\n\n\n\n\n1\n\\theta_1\n0\n90\n55\n\n\n2\n\\theta_2\n80\n0\n0\n\n\n3\n\\theta_3\n120\n0\n0\n\n\n\nIn this convention, each homogeneous transformation A_i is represented as a product of the four basic transformations (Spong, Hutchinson, and Vidyasagar 2005), which evaluates to a 4\\times4 matrix that is used to transform a point from frame n to n\\ -1.\n\nA_i = Rot_{z, \\theta_i}\\ Trans_{x, a_i}\\ Rot_{x, \\alpha_i} \\\\\n\n\n= \\begin{bmatrix}\nc_{\\theta_i} & -s_{\\theta_i} & 0 & 0 \\\\\ns_{\\theta_i} & c_{\\theta_i} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & d_i \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\\\\\n\\times \\begin{bmatrix}\n1 & 0 & 0 & a_i \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & c_{\\alpha_i} & -s_{\\alpha_i} & 0 \\\\\n0 & s_{\\alpha_i} & c_{\\alpha_i} & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\n\n\\begin{bmatrix}\nc_{\\theta_i} & -s_{\\theta_i}c_{\\alpha_i} & s_{\\theta_i}c_{\\alpha_i} & a_ic_{\\theta_i} \\\\\ns_{\\theta_i} & c_{\\theta_i}c_{\\alpha_i} & -c_{\\theta_i}s_{\\alpha_i} & a_is_{\\theta_i} \\\\\n0 & s_{\\alpha_i} & c_{\\alpha_i} & d_i \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\nConsidering the three links of the meArm, the total homogeneous transformation will be a product of the transformations of the three links given as:\n\nA_T = A_1 \\times A_2 \\times A_3\n\\tag{2.1}\nThe final total homogeneous transformation was derived to be:\n\nA_t =\\\n\\begin{bmatrix}\ncos(\\theta_2 + \\theta_3)cos(\\theta_1) & -sin(\\theta_2 + \\theta_3)cos(\\theta_1) & sin(\\theta_1) & 4\\sigma_1cos(\\theta_1)\n\\\\cos(\\theta_2 + \\theta_3)sin(\\theta_1) & -sin(\\theta_2 + \\theta_3)sin(\\theta_1) & -cos(\\theta_1) & 4\\sigma_1sin(\\theta_1)\n\\\\\nsin(\\theta_2 + \\theta_3) & cos(\\theta_2 + \\theta_3) & 0 & 12sin(\\theta_2 + \\theta_3) + 8sin(\\theta_2) - \\frac{11}{2}\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\tag{2.2}\nwhere\n\n\\sigma_1 = 3\\ cos(\\theta_2 + \\theta_3) + 2 \\ cos(\\theta_2)\n\nThe position and orientation of the end effector x, y, z of the meArm was consequently obtained from the total homogeneous A_ttransformation using the upper right 3x1 matrix as:\n\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz\\end{bmatrix} =\n\\begin{bmatrix}\n4\\cos(\\theta_1) \\ (3\\ cos(\\theta_2 + \\theta_3) + 2 \\ cos(\\theta_2)) \\\\\n4\\sin(\\theta_1) \\ (3\\ cos(\\theta_2 + \\theta_3) + 2 \\ cos(\\theta_2)) \\\\\n12sin(\\theta_2 + \\theta_3) + 8sin(\\theta_2) - \\frac{11}{2}\n\\end{bmatrix}\n\\tag{2.3}\n\n\n\n\n\n\nTip\n\n\n\nSee this paper for a thorough derivation of the above.\n\n\nThat said, let’s put this into code:\n\n# Function that calculates forward kinematics\nfkin <- function(motor_angles){\n  \n  # Convert to radians\n  angles = motor_angles * pi/180\n  \n  # Extract angles\n  theta1 = angles[1] \n  theta2 = angles[2]\n  theta3 = pi - angles[3]\n  \n  # Calculate x, y, z\n  x <- 4 * cos(theta1) *( (3*cos(theta2 + theta3)) + (2*cos(theta2)))\n  \n  y <- 4 * sin(theta1) *( (3*cos(theta2 + theta3)) + (2*cos(theta2)))\n  \n  z <- (12 * sin(theta2 + theta3)) + (8*sin(theta2)) - (11/2)\n  \n  \n  # Return a tibble\n  fkin <- tibble(\n    \n    orientation = c(\"x\", \"y\", \"z\"),\n    \n    # Multiply by -1 to re-orient y and z\n    # mistake made during finding DH\n    \n    position = round(c(x, y * -1, z * -1))\n    \n  )\n  \n  return(fkin)\n  \n  \n  \n\n}\n\nWhat would be the x, y, z coordinates of the end effector when rotation of motors 1, 2, 3 are 90, 113, 78 degrees respectively?\n\n# Calculate forward kinematics\nfkin(motor_angles = c(90, 113, 78))\n\n# A tibble: 3 x 2\n  orientation position\n  <chr>          <dbl>\n1 x                  0\n2 y                 13\n3 z                  5\n\nfkin(motor_angles =c(42, 110, 114))\n\n# A tibble: 3 x 2\n  orientation position\n  <chr>          <dbl>\n1 x                -11\n2 y                 10\n3 z                 -3\n\n\n\n\n2.1.2 Inverse kinematics\nInverse kinematics addresses the more difficult converse problem of computing the set of joint angles that will place the end effector at a desired position and orientation. It is the computation of the manipulator joint angles given the position and orientation of the end effector.\nIn solving the inverse kinematics problem, the Geometric approach was used to decompose the spatial geometry into several plane-geometry problems based on the sine and the cosine rules. This was done by considering the trigonometric decomposition of various planes of the manipulator as graphically illustrated below:\n\n\n\n\n\n\n\n\n\n\n\n\\theta_1 = tan^{-1} \\frac{y}{x}\n\\tag{2.4}\nWith the hypotenuse r, connecting x and y obtained using the Pythagoras theorem as\n\nr = \\sqrt{x^2 + y^2}\n\nThe angles \\theta_2 𝑎𝑛𝑑 \\theta_3 were obtained by considering the plane formed by the second and third links as illustrated:\n\n\\theta_2 = \\alpha + \\beta \\\\\n\\theta_2 = tan^{-1} \\frac{s}{r} \\ + tan^{-1} \\frac{l_3sin(\\theta_3)}{l_2 + l_3cos(\\theta_3)}\n\\tag{2.5}\n\n\\theta_3 = cos^{-1}\\frac{x^2 + y^2 +s ^2 - l_2^2 - l_3^2}{2l_2l_3}\n\\tag{2.6}\nWhere s is the difference between the distance of the end effector from the base and the offset: $$ s = z- d\n$$\nAgain, let’s put the above into code\n\nikin <- function(xyz_coordinates){\n  \n  # Extract xyz coordinates\n  x = xyz_coordinates[1]\n  y = xyz_coordinates[2]\n  z = xyz_coordinates[3]\n  \n  # Account for manipulator moving right or left\n  if (x >= 0){\n    theta1 = atan(x/y) + pi/2\n  } else {\n    theta1 = atan(y/x) %>% abs()\n  }\n  \n  # Calculate theta 3 since its needed in theta 2\n  theta3 = acos((x^2 + y^2 + (z-5)^2 - 8^2 - 12^2) / (2*8*12))\n    # 8 and 12 are the dimensions of manipulator arms\n  \n  \n  # Calculate theta 2\n  theta2 = atan((5.5 - z) / (sqrt(x^2 + y^2)) ) + atan((12 * sin(theta3)) / (8 + 12*cos(theta3)))\n  \n  if(theta2 > 0){\n    theta2 = pi - abs(theta2)\n  }\n  \n  tbl <- tibble(\n    ef_position = c(x, y, z),\n    motor_angles = (c(theta1, theta2, pi-theta3)*180/pi) %>% round()\n  )\n  \n  return(tbl)\n  \n}\n\nTheoretically, the results from inverse kinematics and forward kinematics should be in tandem for a given set of inputs. Let’s see whether we can get our previous joint angles from the results of the forward kinematics.\n\n# Calculate inverse kinematics\nikin(xyz_coordinates = c(0, 13, 5))\n\n# A tibble: 3 x 2\n  ef_position motor_angles\n        <dbl>        <dbl>\n1           0           90\n2          13          113\n3           5           78\n\nikin(xyz_coordinates = c(-11, 10, -3))\n\n# A tibble: 3 x 2\n  ef_position motor_angles\n        <dbl>        <dbl>\n1         -11           42\n2          10          110\n3          -3          114"
  },
  {
    "objectID": "02_kinematics.html#summary",
    "href": "02_kinematics.html#summary",
    "title": "2  Manipulator kinematics",
    "section": "2.2 Summary",
    "text": "2.2 Summary\nFrom the above examples, the xyz coordinate values obtained from forward kinematics operation produced the same input angles when passed through the inverse kinematics equations. This will be essential for future operations such as validating whether motor angle rotations result in a desired xyz position of the end effector.\nAnd with that, this section is done! Please do feel free to reach out in case of any questions, feedback and suggestions.\nHappy LeaRning,\nEric.\n\n\n\n\n\n\nSpong, M. W., S. Hutchinson, and M. Vidyasagar. 2005. Robot Modeling and Control. Wiley."
  },
  {
    "objectID": "03_board_mapping.html",
    "href": "03_board_mapping.html",
    "title": "\n3  Board mapping\n",
    "section": "",
    "text": "This notebook illustrates how we assigned real world board coordinates (cm) to each square box on the chess board.\nThe approach was quite straightforward. We created a virtual board and then translated virtual coordinates to the manipulator’s workspace."
  },
  {
    "objectID": "03_board_mapping.html#creating-virtual-chessboard.",
    "href": "03_board_mapping.html#creating-virtual-chessboard.",
    "title": "\n3  Board mapping\n",
    "section": "\n3.1 Creating virtual chessboard.",
    "text": "3.1 Creating virtual chessboard.\n\nlibrary(here)\nlibrary(waffle)\nlibrary(patchwork)\nlibrary(magick)\nlibrary(tidyverse)\n\n# Vector\nx <- c(A= 1, B = 1, C = 1, D = 1, E = 1, F = 1, G = 1)\ny <- c(A= 1, B = 1, C = 1, D = 1, E = 1, F = 1, G = 1)\n\n# Create checker boxes\nw1 = waffle(x, rows = 8, flip = TRUE, colors = c(\"black\", \"white\", \"black\", \"white\", \"black\", \"white\", \"black\", \"white\"), legend_pos = \"\", , size = 0.1) + \n  theme(plot.margin = margin(0, 0, 0, 0))\nw2 = waffle(y, rows = 8, flip = TRUE, colors = c(\"white\", \"black\", \"white\", \"black\", \"white\", \"black\", \"white\", \"black\"), legend_pos = \"\", size = 0.1) +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\n# Make checker board\ncheckerboard <- w1 / w2 / w1 / w2 / w1 / w2 / w1 / w2\ncheckerboard\n\n\n\nggsave(\"images/checkerboard.png\", width = 7, height = 7)"
  },
  {
    "objectID": "03_board_mapping.html#convert-image-into-a-tibble",
    "href": "03_board_mapping.html#convert-image-into-a-tibble",
    "title": "\n3  Board mapping\n",
    "section": "\n3.2 Convert image into a tibble",
    "text": "3.2 Convert image into a tibble\n\nlibrary(magick)\nimg <- image_read(\"images/checkerboard.png\") %>% \n  image_convert(type = \"grayscale\")\n# Dimensions of virtual board\ndim_x = 160 * 4\ndim_y = 160 * 4\nimg <- image_resize(img, paste(dim_x, dim_y, sep = \"x\"))\n\n\n# Create row and column identifiers\nrow_names <-  tibble(x = 1:dim_x, y = rep(LETTERS[1:8], each = dim_y/8), z = paste(x, \"_\", y, sep = \"\")) %>% pull(z)\n\ncol_names <-  tibble(x = 1:dim_x, y = rep(1:8, each = dim_x/8), z = paste(x, \"_\", y, sep = \"\")) %>% pull(z)\n\n# Create array and number rows and columns\nimg_array <- drop(as.integer(pluck(img, 1)))\nrownames(img_array) <- row_names\ncolnames(img_array) <- col_names\n\n\n# Create data frame from array and rename column\nimg_dfx <- img_array %>% \n  as_tibble() %>% \n  mutate(y = row_names) %>% \n  #rowid_to_column(var = \"y\") %>% \n  pivot_longer(!y, names_to = \"x\", values_to = \"pv\") %>% \n  mutate(pv = scales::rescale(pv, to = c(0, 1))) %>% \n  # binarize image\n  mutate(pv = case_when(\n    pv > 0.5 ~ 1,\n    TRUE ~ 0)) %>% \n  separate(y, c(\"y\", \"pl\")) %>% \n  separate(x, c(\"x\", \"pn\")) %>% \n  mutate(pos = paste(pl, pn, sep = \"\")) %>% \n  select(-c(pn, pl)) %>% \n  mutate(across(c(y, x, pv), as.numeric)) %>% \n  group_by(pos) %>% \n  mutate(centroidx = round(mean(x)), centroidy = round(mean(y))) %>% \n  ungroup()\n\nOnly centroid locations are of importance. So we narrow down to that:\n\n# Obtain location centroids\ncentroids <- img_dfx %>% \n  ungroup() %>% \n  distinct(pos, centroidx, centroidy)\n\n# View centroids on the virtual board\ncentroids %>% \n  slice_head(n = 10)\n\n# A tibble: 10 x 3\n   pos   centroidx centroidy\n   <chr>     <dbl>     <dbl>\n 1 A1           40        40\n 2 A2          120        40\n 3 A3          200        40\n 4 A4          280        40\n 5 A5          360        40\n 6 A6          440        40\n 7 A7          520        40\n 8 A8          600        40\n 9 B1           40       120\n10 B2          120       120"
  },
  {
    "objectID": "03_board_mapping.html#mapping-coordinates-to-xy",
    "href": "03_board_mapping.html#mapping-coordinates-to-xy",
    "title": "\n3  Board mapping\n",
    "section": "\n3.3 Mapping coordinates to xy",
    "text": "3.3 Mapping coordinates to xy\nOnce the centroid pixel coordinates were successfully extracted, the next step involved translating them to their corresponding real-world x-y coordinates in cm. This scaling from pixel values to cm values was achieved by the mapping function or simply using scales::rescale():\n\n# Mapping function\nmap_fun <- function(value, from_low, from_high, to_low, to_high){\n  \n  mapped_val = (value - from_low) * (to_high - to_low) / (from_high - from_low) + (to_low)\n  \n  return(mapped_val)\n}\n\n\n# Map virtual board coordinates to manipulator workspace\ncentroids = centroids %>% \n    # mutate(x_mapped = map_fun(centroidx, from_low = 0, from_high = dim_x, to_low = 9, to_high = -9),\n    #        y_mapped = map_fun(centroidy, from_low = 0, from_high = 192, to_low = 11, to_high = 17)) %>% \n    \n    ## Using scales::rescale\n    mutate(x_mapped = scales::rescale(centroidx, to = c(9, -9), from = c(0, 640)),\n           y_mapped = scales::rescale(centroidy, to = c(11, 17), from = c(0, 192))) %>% \n    mutate(across(where(is.numeric), round)) %>% \n    # Rearrange board positions to match our physical chess board\n    mutate(\n      pl = rep(LETTERS[1:8], times = nrow(centroids)/8),\n      pn = rep(1:8, each = nrow(centroids)/8),\n      pos = paste(pl, pn, sep = \"\")) %>% \n    select(-c(pl, pn))\n\n# View centroids coordinates\ncentroids %>% \n  slice_head(n = 10)\n\n# A tibble: 10 x 5\n   pos   centroidx centroidy x_mapped y_mapped\n   <chr>     <dbl>     <dbl>    <dbl>    <dbl>\n 1 A1           40        40        8       12\n 2 B1          120        40        6       12\n 3 C1          200        40        3       12\n 4 D1          280        40        1       12\n 5 E1          360        40       -1       12\n 6 F1          440        40       -3       12\n 7 G1          520        40       -6       12\n 8 H1          600        40       -8       12\n 9 A2           40       120        8       15\n10 B2          120       120        6       15\n\n\nNow let’s visualize our handy work:\n\ntheme_set(theme_void())\n\n# Create virtual board\ncentroids %>% \n  ggplot(mapping = aes(x = centroidx*2, y = centroidy*2)) +\n  geom_tile(aes(fill = str_extract(pos, \"[:alpha:]\")), color = \"white\", alpha = 0.8, size = 0.8, show.legend = FALSE) +\n  geom_text(aes(x = centroidx*2, y = centroidy*2 + 2, label = pos), color = \"black\", size = 3.5) +\n  coord_equal() +\n  #paletteer::scale_fill_paletteer_d(\"RColorBrewer::Set2\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"RColorBrewer::Set2\") %>% paste() %>% str_replace(\"#FFD92FFF\", \"#FFC000FF\"))\n\n\n\n# Virtual coordinates on board\ncentroids %>% \n  ggplot(mapping = aes(x = centroidx*2, y = centroidy*2)) +\n  geom_tile(aes(fill = str_extract(pos, \"[:alpha:]\")), color = \"white\", alpha = 0.8, size = 0.8, show.legend = FALSE) +\n  geom_text(aes(x = centroidx*2, y = centroidy*2 + 2, label = paste(\"(\", centroidx, \",\", centroidy, \")\", sep = \"\")), color = \"black\", size = 3.3) +\n  coord_equal() +\n  #paletteer::scale_fill_paletteer_d(\"RColorBrewer::Set2\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"RColorBrewer::Set2\") %>% paste() %>% str_replace(\"#FFD92FFF\", \"#FFC000FF\"))\n\n\n\n# Real world coordinates on board\ncentroids %>% \n  ggplot(mapping = aes(x = centroidx*2, y = centroidy*2)) +\n  geom_tile(aes(fill = str_extract(pos, \"[:alpha:]\")), color = \"white\", alpha = 0.8, size = 0.8, show.legend = FALSE) +\n  geom_text(aes(x = centroidx*2, y = centroidy*2 + 2, label = paste(\"(\", x_mapped, \",\", y_mapped, \")\", sep = \"\")), color = \"black\", size = 3.3) +\n  coord_equal() +\n  #paletteer::scale_fill_paletteer_d(\"RColorBrewer::Set2\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"RColorBrewer::Set2\") %>% paste() %>% str_replace(\"#FFD92FFF\", \"#FFC000FF\"))\n\n\n\n\nHere’s an example of how we would extract the real word coordinates from the tibble:\n\n# Function to return mapped values from tibble\nget_centroid <- function(position){\n  x = centroids %>% \n    filter(str_detect(pos, position)) %>% \n    pull(x_mapped)\n  \n  y = centroids %>% \n    filter(str_detect(pos, position)) %>% \n    pull(y_mapped)\n  \n  return(c(x, y))\n}\n\n# Define initial and final positions\ninitial_pos <- \"B2\"\nfinal_pos <- \"G2\"\nget_centroid(position = initial_pos)\n\n[1]  6 15\n\nget_centroid(position = final_pos)\n\n[1] -6 15"
  },
  {
    "objectID": "03_board_mapping.html#summary",
    "href": "03_board_mapping.html#summary",
    "title": "\n3  Board mapping\n",
    "section": "\n3.4 Summary",
    "text": "3.4 Summary\nThis is the approach we took at this step. We created a virtual board and then translated virtual coordinates to the manipulator’s workspace.\nAnd with that, this section is done! Please do feel free to reach out in case of any questions, feedback and suggestions.\nHappy Learning,\nEric."
  },
  {
    "objectID": "04_aRduino.html",
    "href": "04_aRduino.html",
    "title": "\n4  R and Arduino\n",
    "section": "",
    "text": "This notebook briefly describes how we set up a communication interface between R and a microcontroller ( Arduino). Most of it is based on a blog post we wrote a while back: What we R about when we R about R and Arduino.\nArduino is an open-source electronics platform based on easy-to-use hardware (Arduino Board) and software (Arduino IDE). One can tell the board what to do if one has the correct form of data and a set of instructions for processing the data and performing subsequent operations. The Arduino’s microcontroller is responsible for holding all your compiled code and executing the commands you specify. The Arduino Software on the other hand is the board’s IDE where one writes the set of instructions governing the board. The getting started guide would be a good place to start learning about the Arduino ecosystem.\nSwitching over to R, we couldn’t have found better words to summarize what R is than with these words found in the book Advanced R by Hadley Wickham: Despite its sometimes frustrating quirks, R is, at its heart, an elegant and beautiful language, well tailored for data science 🤗.\nWith all this said, a fine convergence can be struck between the two: data. Consider this very simple example. We want the Arduino board to turn an LED (Light Emitting Diode) ON once it receives a 1 and OFF once it receives a 0. If one can get a way of sending some data (1 or 0) to the board’s microcontroller, then, the set objective will be achieved sooner or later.\n\nSerial communication is the communication protocol that will be used between R and the Arduino software similar to what is used in the Arduino serial monitor. This communication interface will facilitate the transmission of data between the two interfaces.\nThe serial package (Seilmayer 2020) will be used to set up this comunication interface.\nLet’s begin by loading the required packages.\n\nlibrary(tidyverse)\nlibrary(serial)\nlibrary(here)\n\nNext, we’ll create a serial port object called arduino, which represents a serial client for communication with the USB serial port where our board is connected.\n\n# See the ports available\nlistPorts()\n\nCreate an Arduino object and set up the interface parameters.\nThis is achieved using the serial::serialConnection function. The interface parameters are such that the baud rate (specifies the number of bits being transferred per second) is set to 9600, which is the same value in the Arduino script. Also, we have specified that the transmission ends with a new line and that the transmission is complete if the end of line symbol is the carriage return cr.\n\narduino <-  serialConnection(name = \"aRduino\",\n                           port = \"COM5\",\n                           mode = \"9600,n,8,1\" ,\n                           buffering = \"none\",\n                           newline = TRUE,\n                           eof = \"\",\n                           translation = \"cr\",\n                           handshake = \"none\",\n                           buffersize = 8096\n                           \n                           )\n\nNow that the serial interface is in place, the next step is initialising the interface and keeping it open for later usage such as writing and reading data from it. Once serial::isOpen initialises the interface, the Arduino board blinks. This is because the board resets once a serial port is opened to allow the bootloader to receive a new sketch.\nserial::isOpen tests whether the connection is open or not.\n\nopen(arduino)\n\n# testing whether the connection is open or not\nisOpen(arduino)\n\n\nAt this point, we are all set to write some data to the serial interface.\nLet’s prepare some data to send to the serial interface.\n\n## Create dummy data\nn = 42\narduino_input <- tibble(\n  c = sample(10:100, size = n, replace = T) %>%\n                     paste('C', sep = ''))\n\nThe chunk below uses serial::write.serialConnection() to write the LED values to the serial port row by row.\n\nclose(arduino)\nopen(arduino)\nSys.sleep(3)\nfor (r in 1:nrow(arduino_input)){\n  Sys.sleep(0.3)\n  write.serialConnection(arduino, paste(arduino_input[r,], collapse = ''))\n}\nSys.sleep(2)\n\n\nThe Arduino board itself is a programmable platform. You can tell your board what to do by sending a set of instructions to the microcontroller on the board. To do so you use the Arduino programming language and the Arduino Software (IDE). Here are sample instructions that were uploaded to the microcontroller:\n\nif(Serial.available()){ // checks data in serial\n\n  static int t=0;\n\n    char mychar=Serial.read(); // reads serial data\n\n    switch(mychar){      \n\n      case '0'...'9':\n\n        t=t*10 + mychar - '0’; // parse integers​\n\n        break;\n\n      case 'A':\n\n        {\n\n            servoA.write(t,50,1); // write value to motor\n\n            Serial.println(t); // print data to serial\n\n        }\n\n        t=0;​\n\n        break;​\n\n        ...​\n\n    }\nOn a very high level, the microcontroller:\n\nChecks whether data is available on the serial interface.\nReads the data one byte at a time\n\nUses a series of switch case commands to\n\nParse integers\nWrite motor angles (send an electrical signal) to each respective motor based on the motor’s tag e.g A, B or C\n\n\n\nSo how do the servo motors actually rotate? Servos are controlled using adjustable pulse widths on the signal line. This is achieved using a technique called Pulse Width Modulation. PWM is a modulation technique that generates variable-width pulses to represent the amplitude of an analog input signal.\nFor a standard servo, sending a 1 ms 5V pulse turns the motor to 0 degrees, and sending a 2 ms 5V pulse turns the motor to 180 degrees, with pulse lengths in the middle scaling linearly. A 1.5 ms pulse, for example, turns the motor to 90 degrees. Once a pulse has been sent, the servo turns to that position and stays there until another pulse instruction is received. However, if you want a servo to “hold” its position (resist being pushed on and try to maintain the exact position), you just resend the command once every 20 ms. The Arduino servo commands e.g servo.write takes care of all this for you. To better understand how servo control works, please see the timing diagram:\n\n\nServo motor timing diagram: Jeremy Blum - Exploring Arduino\n\n\nA great place to get started with Arduino and some hobby electronics projects would be Blum (2013).\n\nWe can read the values sent to the serial port connection by Arduino script using read.serialConnection()\n\ndata_frm_arduino <- tibble(capture.output(cat(read.serialConnection(arduino)))) %>% \n  filter(if_any(where(is.character), ~ .x != \"\"))\n\n\ndata_frm_arduino\n\n\nThere we go! In this section, we leveraged Arduino’s capability to be programmed via a serial interface to send and receive data from R to the Arduino board.\nPlease do feel free to reach out in case of any questions, feedback and suggestions.\nHappy Learning,\nEric.\n\n\n\n\n\n\nBlum, J. 2013. Exploring Arduino: Tools and Techniques for Engineering Wizardry. EEET 2046 : Electronics. Wiley. https://books.google.co.uk/books?id=iXsQAAAAQBAJ.\n\n\nSeilmayer, Martin. 2020. “Serial: The Serial Interface Package.” https://CRAN.R-project.org/package=serial."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "The instructions on the microcontroller (described in chapter 4) that send electrical pulses to rotate our motors can be found in the file robot_arm_control.ino.\nAnd that’s it! There goes a touch of R in robotics.\nPlease do feel free to reach out in case of any questions, feedback and suggestions,\nEric and Ian."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blum, J. 2013. Exploring Arduino: Tools and Techniques for\nEngineering Wizardry. EEET 2046 : Electronics. Wiley. https://books.google.co.uk/books?id=iXsQAAAAQBAJ.\n\n\nSeilmayer, Martin. 2020. “Serial: The Serial Interface\nPackage.” https://CRAN.R-project.org/package=serial.\n\n\nSpong, M. W., S. Hutchinson, and M. Vidyasagar. 2005. Robot Modeling\nand Control. Wiley."
  }
]